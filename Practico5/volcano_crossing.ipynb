{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctico 5: Volcano Crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcano_crossing_env import VolcanoCrossing\n",
    "\n",
    "env = VolcanoCrossing(slip_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game():\n",
    "        print(\"Play manually...\")\n",
    "        obs = env.reset()\n",
    "        print(obs)\n",
    "        done = False\n",
    "        step_counter = 0\n",
    "        all_rewards = 0\n",
    "        env.render()\n",
    "\n",
    "        while not done:\n",
    "            action = input(\"Next action: \")\n",
    "            env.check_action(action)\n",
    "            obs, reward, done_env, _ = env.step(action)\n",
    "            print(f'{obs=} {reward=} {done_env=}')\n",
    "            all_rewards += reward\n",
    "            done = done_env\n",
    "            env.render()\n",
    "            step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar con una policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_south(state) :\n",
    "  return 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy) :\n",
    "  U = 0\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  while not done:\n",
    "    state, reward, done, _ = env.step(policy(state))\n",
    "    U += reward\n",
    "  return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(policy_south)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de la policy por promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0\n",
    "N = 50_000\n",
    "for i in range(N):\n",
    "    r += run(policy_south)\n",
    "\n",
    "print (f'Average reward: {r/N}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, delta = 0.05, gamma = 0.999):\n",
    "  V = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  V_new = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  max_diff = delta + 1\n",
    "\n",
    "  while max_diff > delta:\n",
    "    for s in env.observation_space:\n",
    "      a = policy(s)\n",
    "      q = 0\n",
    "      for s_prime in env.P[s][a].keys():\n",
    "        q += env.P[s][a][s_prime] * (env.R[s][a][s_prime] + gamma * V[s_prime])\n",
    "      V_new[s] = q\n",
    "    max_diff = 0\n",
    "    for s in V.keys():\n",
    "      max_diff = max(max_diff, abs(V[s]-V_new[s]))\n",
    "    V = V_new.copy()\n",
    "  return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(policy_south, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = { '11':'S', '12':'S', '13':'S', '14':'S', '21':'S', '22':'S', '23':'S', '24':'S', '31':'S', '32':'S', '33':'S', '34':'S' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar un paso de policy improvement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar policy iteration y ver cuál es la mejor acción para cada estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "1. Implementar el algoritmo de Value Iteration para encontrar la policy óptima.\n",
    "2. Obtener la utilidad esperada de esa policy.\n",
    "3. Graficar el valor del estado \"21\" para cada iteración en Value Iteration y Policy Iteration (con una policy inicial arbitraria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, delta_threshold=0.05, gamma=0.999):\n",
    "    pass\n",
    "\n",
    "def q_value(mdp, state, action, V, gamma):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
